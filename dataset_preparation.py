# -*- coding: utf-8 -*-
"""Dataset_preparation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13OmrMG5dtidnVFXX1HeZfMW3psBh91yE
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
from tqdm import tqdm
import re
import pickle
import json
from sklearn.model_selection import train_test_split
import random

with open('/content/drive/MyDrive/AAAI work/knowledge graph/disease_symptom.p', 'rb') as f:
    x = pickle.load(f)

lst=[]

for i, diseases in enumerate(x):
  tmp = list(x[diseases]['symptom'].keys())
  lst = lst+tmp

symptom_list=list(set(lst))
symptom_list = list(map(lambda x: x.lower(), symptom_list))

disease_names = list(x.keys())
disease_names = list(map(lambda x: x.lower(), disease_names))

df1 = pd.read_csv('/content/drive/MyDrive/AAAI work/dataset/Final_SD Dataset [with Empathy & Severity] - SD [punctuation considered in BIO tag].csv').astype(str)
df1.columns=(['patient_dialogue', 'doctor_dialogue', 'bio_tagging', 'intent_type', 'img_info', 'severity'])
df1['punctuation_considered']='yes'

df2 = pd.read_csv('/content/drive/MyDrive/AAAI work/dataset/Final_SD Dataset [with Empathy & Severity] - SD2 [punctuation not considered in BIO tag].csv').astype(str)
df2 = df2.drop(['Unnamed: 6'], axis=1)
df2 = df2.iloc[: , :6]
df2.columns=(['patient_dialogue', 'doctor_dialogue', 'bio_tagging', 'intent_type', 'img_info', 'severity'])
df2['patient_dialogue'][0] = 'Patient'

df2.columns=(['patient_dialogue', 'doctor_dialogue', 'bio_tagging', 'intent_type', 'img_info', 'severity'])
df2['punctuation_considered']='no'
df = df1.append(df2, ignore_index=True)
df=df[df['patient_dialogue']!='nan']
df=df.reset_index(drop=True)
df['patient_dialogue'][6168]='Dialogue ID'

dialogues=[]
utterances=[]
dialogue_id_list=[]
bio_tagging=[]
intent_type=[]
img_info=[]
severity=[]

tmp_bio_tagging=[]
tmp_intent_type=[]
tmp_img_info=[]
tmp_severity=[]

for i in range(len(df)):
  if df['patient_dialogue'][i].lower() == 'dialogue id':
    dialogue_id_list.append(df['doctor_dialogue'][i])

for i in tqdm(range(len(df))):

  if df['patient_dialogue'][i].lower()!='dialogue id':

    utterances.append(df['patient_dialogue'][i].strip())
    utterances.append(df['doctor_dialogue'][i].strip())
    tmp_bio_tagging.append(df['bio_tagging'][i].strip())
    tmp_intent_type.append(df['intent_type'][i].lower())
    tmp_img_info.append(df['img_info'][i].lower())
    tmp_severity.append(df['severity'][i].lower())


  else:
    dialogues.append(utterances[:-1][2:])
    utterances=[]
    bio_tagging.append(tmp_bio_tagging[:-1][1:])
    tmp_bio_tagging=[]
    intent_type.append(tmp_intent_type[:-1][1:])
    tmp_intent_type=[]
    img_info.append(tmp_img_info[:-1][1:])
    tmp_img_info=[]
    severity.append(tmp_severity[:-1][1:])
    tmp_severity=[]

dialogues.append(utterances)
bio_tagging.append(tmp_bio_tagging)
intent_type.append(tmp_intent_type)
img_info.append(tmp_img_info)
severity.append(tmp_severity)


for i in range(len(dialogues)):
  
  if 'nan' in dialogues[i]:
    dialogues[i].remove('nan')
  if '.' in dialogues[i]:
    dialogues[i].remove('.')
  if '' in dialogues[i]:
    dialogues[i].remove('')

  if 'Patient' in dialogues[i]:
    dialogues[i].remove('Patient')
  if 'Doctor' in dialogues[i]:
    dialogues[i].remove('Doctor')
  

  for j in range(len(dialogues[i])):
    if 'disease' not in dialogues[i][j].lower() and 'group' in dialogues[i][j].lower():
      text = dialogues[i][j]
      dialogues[i].remove(text)

sum=0
disease_list=[]
for i in range(len(dialogues)):
  if 'disease' in dialogues[i][len(dialogues[i])-1].lower():
    sum=sum+1

  disease = dialogues[i][len(dialogues[i])-1].lower()
  if 'group' in disease:
    disease = disease.replace('group', '')#removing the word group
  disease = disease.split('disease')[1]#removing word disease
  disease = re.sub(r'[^\w\s]', '', disease)#removing punctuations
  disease = re.sub(" \d+", " ", disease)#removing digits
  disease = disease.strip()#removing white spaces
  
  if disease == 'chagas':
    disease='chagas disease'
  if disease == 'cap scratch' or disease=='cat scratch':
    disease = 'cat scratch disease'
  if disease == 'chicken pox':
    disease = 'chickenpox'
  if disease == 'eczema coma':
    disease = 'eczema'
  if disease == 'carcinoma syndrome':
    disease = 'carcinoid syndrome'
  if disease == 'guillan barre syndrome' or disease == 'guillian bare syndrome' or disease == 'guillian barre syndrome':
    disease = 'guillain barre syndrome'
  if disease == 'extrapyramidal effects of drugs' or disease == 'extra pyramidal effect of drugs':
    disease = 'extrapyramidal effect of drugs'
  if disease == 'alzheimer':
    disease = 'alzheimer disease'
  if disease == 'chronic pain':
    disease = 'chronic pain disorder'
  if disease == 'chronic kidney':
    disease = 'chronic kidney disease'
  if disease == 'de quervain':
    disease = 'de quervain disease'
  if disease == 'fibrocystic breast':
    disease = 'fibrocystic breast disease'
  if disease == 'degenerative disc':
    disease = 'degenerative disc disease'
  if disease == 'graves':
    disease = 'graves disease'


  disease_list.append(disease)
disease_list = list(map(lambda x:x.lower(), disease_list))

i

for i in range(len(dialogues)):
  dialogues[i] = dialogues[i][:-1]
dialogues[2]=['Hi! Doctor, I am suffering from Foreign body sensation in eye. Would you please tell me what could be its cause?',
              'Definitely, I will. Per the symptoms, you might have Diabetic retinopathy.']
              
a = [1059]
dialogue_id_list = a+dialogue_id_list

data = []
for i in range(len(dialogues)):
  data.append({'Dialogue_ID':dialogue_id_list[i],
               'Dialogues':dialogues[i], 
               'Disease':disease_list[i],
               'BIO_tagging':bio_tagging[i],
               'Intent_type':intent_type[i],
               'Image_info':img_info[i],
               'Severity':severity[i]})
               #'Visual_symptom_present':'no'})
data[2]['BIO_tagging']=['O O O O O O O O B-Symptom I-Symptom I-Symptom I-Symptom I-Symptom O O O O O O O O O O O O']
data[960]['BIO_tagging'] = data[960]['BIO_tagging'][:-1]
data[2]['Image_info']=['nan']

for i, samples in enumerate(data):
  tmp_vs='no'
  for j, vs in enumerate(samples['Dialogues']):
    if '$' in vs:
      tmp_vs='yes'
      break
  data[i]['Visual_symptom_present'] = tmp_vs

infile = open('/content/drive/MyDrive/AAAI work/predicted_img_labels/list_tuples_softmax_outputs_all_images.p','rb')
img_tuples = pickle.load(infile, encoding='bytes')

id_to_labels={0:0}
for i in range(len(img_tuples)):
  id = img_tuples[i][0].split('/')[1]
  labels = img_tuples[i][1]
  id_to_labels[id]=labels
id_to_labels.pop(0)

infile = open('/content/drive/MyDrive/AAAI work/predicted_img_labels/list_tuples_severity_outputs_all_images.p','rb')
sev_tuples = pickle.load(infile, encoding='bytes')

id_to_severity={0:0}
for i in range(len(sev_tuples)):
  id = sev_tuples[i][0].split('/')[1]
  sev = sev_tuples[i][2]
  if sev==1:
    txt='severe'
  elif sev==2:
    txt='moderate'
  else:
    txt='mild'
  
  id_to_severity[id]=txt
id_to_severity.pop(0)

"""# Knowledge graph

"""

with open(r"/content/drive/MyDrive/AAAI work/knowledge graph/SD++.p", "rb") as input_file:
  kg = pickle.load(input_file)

#removing disease keys
for i in list(kg.keys()):
  if 'D' in i:
    kg.pop(i)

#sorting according to coorelation value
for i in list(kg.keys()):
  kg[i] = sorted(kg[i].items(), key=lambda x: x[1], reverse=True)

tmp_lst=[]
lst=[]

for i in kg.keys():
  for j in range(len(kg[i])):
    tmp_lst.append([i,kg[i][j][0],kg[i][j][1]])
  lst.append(tmp_lst)

  tmp_lst=[]

t=[]
r=[]
count=0
for i, samples in enumerate(lst):
  for j, k in enumerate(samples):
    if 'S' in k[1] and count<3:
      r.append(k)
      count=count+1
  t.append(r)
  r=[]
  count=0
top_symptom = t

t=[]
r=[]
count=0
for i, samples in enumerate(lst):
  for j, k in enumerate(samples):
    if 'D' in k[1] and count<3:
      r.append(k)
      count=count+1
  t.append(r)
  r=[]
  count=0
top_disease = t

with open(r"/content/drive/MyDrive/AAAI work/Disease and symptom mapping/IDD.p", "rb") as input_file:
  disease_mapping = pickle.load(input_file)

with open(r"/content/drive/MyDrive/AAAI work/Disease and symptom mapping/IDS.p", "rb") as input_file:
  symptom_mapping = pickle.load(input_file)

for i, tup in enumerate(top_symptom):
  for j, k in enumerate(tup):
    top_symptom[i][j][1] = symptom_mapping[top_symptom[i][j][1]]
    top_symptom[i][j][0] = symptom_mapping[top_symptom[i][j][0]]
    top_disease[i][j][1] = disease_mapping[top_disease[i][j][1]]
    top_disease[i][j][0] = symptom_mapping[top_disease[i][j][0]]

tmp_slist=[]
tmp_dlist=[]

kgs={0:0}
kgd={0:0}

for i in range(len(top_symptom)):
  for j in range(3):
    tmp_slist.append(top_symptom[i][j][1:2])
    tmp_dlist.append(top_disease[i][j][1:2])

  kgs[top_symptom[i][0][0]] = tmp_slist
  kgd[top_disease[i][0][0]] = tmp_dlist

  tmp_slist=[]
  tmp_dlist=[]

kgs.pop(0)
kgd.pop(0)

for i in kgs.keys():
  string ='[S] ' + kgs[i][0][0] + ' ' + kgs[i][1][0] + ' ' +  kgs[i][2][0] + ' [D] ' + kgd[i][0][0] + ' ' + kgd[i][1][0] + ' ' + kgd[i][2][0]
  kgs[i] = string

symptoms_names=list(set(list(kgs.keys())))

symptoms_names[0]

"""# For first list of metrics"""

### For only predicted symptoms
for i, samples in enumerate(data):

  # for tagging visual symptoms
  if samples['Visual_symptom_present'] == 'yes':
    for j in range(len(samples['Dialogues'])):
      if '$' in samples['Dialogues'][j]:
        tmp = samples['Dialogues'][j]
        tag = (samples['Dialogues'][j].split('$')[1]).split('$')[0]
        sev_tag = samples['Severity'][int(j/2)]
        tmp = tmp.replace(tag, ' ' + sev_tag + ' ' + id_to_labels[tag] + ' ')#(tmp, df['img_info'][i])   
        #tmp = tmp.replace(tag, ' [MASK] ')# for masking visual symptom
        tmp = tmp.replace('$', '')
        data[i]['Dialogues'][j] = tmp
'''
  if i<=947:
    for j in range(len(samples['Dialogues'])):
      if j%2==0:
        text=samples['Dialogues'][j]
        string=''
        pu=re.findall(r"[\w']+|[.,!?;]", samples['Dialogues'][j])
        bt=samples['BIO_tagging'][int(j/2)].split() 

        #BIO tagging to the sentences
        if len(pu)<len(bt):
          length = len(pu)
        else:
          length = len(bt)

        for k in range(length):
          string=string+pu[k]+' ['+bt[k]+'] '

        if j==0:
          knowledge_graph=' [KG] ' + 'blank'
          for s_names in symptoms_names:
            if s_names.lower() in text.lower():
              knowledge_graph=' [KG] '+ kgs[s_names]
          string=string+knowledge_graph
        data[i]['Dialogues'][j]=string

  else:
    for j in range(len(samples['Dialogues'])):    
      string=''
      if j%2==0:
        pu=re.sub(r'[^\w\s]', '', samples['Dialogues'][j])
        pu=pu.split()
        bt=samples['BIO_tagging'][int(j/2)].split() 
        if len(pu)<len(bt):
          length = len(pu)
        else:
          length = len(bt)
        for k in range(length):
          string=string+pu[k]+' ['+bt[k]+'] '

        if j==0:
          knowledge_graph=' [KG] ' + 'blank'
          for s_names in symptoms_names:
            if s_names.lower() in text.lower():
              knowledge_graph=' [KG] '+ kgs[s_names]
          string=string + knowledge_graph
        data[i]['Dialogues'][j]=string   

for i in range(len(data)):
  for  j in range(len(data[i]['Dialogues'])):
    if j%2==0:
      if '_' in data[i]['Dialogues'][j]:
        data[i]['Dialogues'][j] = data[i]['Dialogues'][j].replace('_', ' [SEV] ')

'''
y=[]
for i in range(len(data)):
    y.append(data[i]['Disease'])
train_data, test_data, _, _ = train_test_split(data, y, stratify=y, test_size=0.2, random_state=0)

datasets={'train':train_data,'test':test_data}

y=[]
for i in range(len(data)):
    y.append(data[i]['Disease'])
    
train_data, test_data, _, _ = train_test_split(data, y, stratify=y, test_size=0.2, random_state=0)

datasets={'train':train_data,'test':test_data}

with open('new_file_89'+'.json', "w") as outfile:
    json.dump(datasets, outfile)

for i in train_data:
  if i['Visual_symptom_present']=='yes':
    print(i)

"""# For second list of metrics"""

def file_generator(name, data=data):

  #for only textual symptom data
  if name == '2_textual_data_masked':

    for i, samples in enumerate(data):

      if samples['Visual_symptom_present'] == 'no' and len(samples['Dialogues'])>2:

        len_patient_utterances = int(len(samples['Dialogues'])/2)
        n = random.sample(range(0,len_patient_utterances), 2)
        data[i]['Dialogues'][int(n[0])*2] = 'I do not know'
        data[i]['Dialogues'][int(n[1])*2]=' I do not know'

      if samples['Visual_symptom_present'] == 'yes':
        
        for j in range(len(samples['Dialogues'])):
          if '$' in samples['Dialogues'][j]:
            tmp = samples['Dialogues'][j]
            tag = (samples['Dialogues'][j].split('$')[1]).split('$')[0]
            tmp = tmp.replace(tag, ' ' + id_to_labels[tag] + ' ')#(tmp, df['img_info'][i])
            tmp = tmp.replace('$', '')
            data[i]['Dialogues'][j] = tmp

      if samples['Visual_symptom_present'] == 'yes' and samples['Image_info'].count('nan')>2:
        tmp=[]
        c=samples['Image_info'].count('nan')
        for j in range(len(samples['Image_info'])):
          if samples['Image_info'][j] == 'nan':
            tmp.append(j)
        
        len_patient_utterances = int(len(samples['Dialogues'])/2)
        n = random.sample(range(0, c-1), 2)
        data[i]['Dialogues'][tmp[int(n[0])]*2] = 'I do not know'
        data[i]['Dialogues'][tmp[int(n[1])]*2] = 'I do not know'
        
        for j in range(len(samples['Dialogues'])):
          if '$' in samples['Dialogues'][j]:
            tmp = samples['Dialogues'][j]
            tag = (samples['Dialogues'][j].split('$')[1]).split('$')[0]
            tmp = tmp.replace(tag, ' ' + id_to_labels[tag] + ' ')#(tmp, df['img_info'][i])
            tmp = tmp.replace('$', '')
            data[i]['Dialogues'][j] = tmp
        
    return data

  #for masking 1 textual symptom
  elif name == '1_textual_data_masked':

    for i, samples in enumerate(data):

      if samples['Visual_symptom_present'] == 'no' and len(samples['Dialogues'])>2:

        len_patient_utterances = int(len(samples['Dialogues'])/2)
        n = random.sample(range(0,len_patient_utterances), 1)
        data[i]['Dialogues'][int(n[0])*2] = 'I do not know'

      if samples['Visual_symptom_present'] == 'yes':
        
        for j in range(len(samples['Dialogues'])):
          if '$' in samples['Dialogues'][j]:
            tmp = samples['Dialogues'][j]
            tag = (samples['Dialogues'][j].split('$')[1]).split('$')[0]
            tmp = tmp.replace(tag, ' ' + id_to_labels[tag] + ' ')#(tmp, df['img_info'][i])
            tmp = tmp.replace('$', '')
            data[i]['Dialogues'][j] = tmp

      if samples['Visual_symptom_present'] == 'yes' and samples['Image_info'].count('nan')>2:
        tmp=[]
        c=samples['Image_info'].count('nan')
        for j in range(len(samples['Image_info'])):
          if samples['Image_info'][j] == 'nan':
            tmp.append(j)
        
        len_patient_utterances = int(len(samples['Dialogues'])/2)
        n = random.sample(range(0, c-1), 1)
        data[i]['Dialogues'][tmp[int(n[0])]*2] = 'I do not know'
        
        for j in range(len(samples['Dialogues'])):
          if '$' in samples['Dialogues'][j]:
            tmp = samples['Dialogues'][j]
            tag = (samples['Dialogues'][j].split('$')[1]).split('$')[0]
            tmp = tmp.replace(tag, ' ' + id_to_labels[tag] + ' ')#(tmp, df['img_info'][i])
            tmp = tmp.replace('$', '')
            data[i]['Dialogues'][j] = tmp
          
    return data


  #for visual masking

  elif name == 'visual_data_masked':

    for i, samples in enumerate(data):

      if samples['Visual_symptom_present'] == 'yes':
        for j, utterance in enumerate(samples['Dialogues']):
          if '$' in utterance:
            data[i]['Dialogues'][j] = 'I do not know'

    return data


  # for masking 1 textual symptom and visual data

  elif name == '1_textual_and_visual':

    for i, samples in enumerate(data):

      if samples['Visual_symptom_present'] == 'no' and len(samples['Dialogues'])>2:

        len_patient_utterances = int(len(samples['Dialogues'])/2)
        n = random.sample(range(0,len_patient_utterances), 1)
        data[i]['Dialogues'][int(n[0])*2] = 'I do not know'


      if samples['Visual_symptom_present'] == 'yes' and samples['Image_info'].count('nan')>2:
        tmp=[]
        c=samples['Image_info'].count('nan')
        for j in range(len(samples['Image_info'])):
          if samples['Image_info'][j] == 'nan':
            tmp.append(j)
        
        len_patient_utterances = int(len(samples['Dialogues'])/2)
        n = random.sample(range(0, c-1), 1)
        data[i]['Dialogues'][tmp[int(n[0])]*2] = 'I do not know'
        
        for j in range(len(samples['Dialogues'])):
          if '$' in samples['Dialogues'][j]:
            data[i]['Dialogues'][j] = 'I do not know'

    return data

files=['2_textual_data_masked',
              '1_textual_data_masked',
              'visual_data_masked',
              '1_textual_and_visual']

for dataset_type in files:
              
  required_data = file_generator(dataset_type)

  y=[]
  for i in range(len(data)):
    y.append(required_data[i]['Visual_symptom_present'])
    
  train_data, test_data, _, _ = train_test_split(required_data, y, stratify=y, test_size=0.2, random_state=0)

  datasets={'train':train_data,'test':test_data}

  with open("/content/drive/MyDrive/AAAI work/Samples for training and testing/"+dataset_type+'.json', "w") as outfile:
      json.dump(datasets, outfile)

len(train_data)/len(test_data)

data[0]