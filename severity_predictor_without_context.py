# -*- coding: utf-8 -*-
"""Severity_predictor_without_context.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1de8UueLoeZOhpgBwu1r9yxJm0xJNE0mw
"""

from google.colab import drive
drive.mount('/content/drive')

dict_count_files_train = {}
d2 = {'S1':'Swollen eye', 'S108':'Mouth ulcer', 'S11':'Skin dryness, peeling, scaliness, or roughness', 
          'S159':'Neck swelling', 'S177': 'Skin irritation','S181':'Swollen or red tonsils', 
          'S205':'Knee swelling', 'S22': 'Redness in ear' , 'S251': 'Dry or flaky scalp','S31': 'Skin rash', 
          'S37':'Itchy eyelid','S39': 'Hand or finger lump or mass','S4': 'Skin growth','S5' : 'Eye redness', 
          'S50':'Foot or toe swelling', 'S51':'Lip swelling', 'S74':  'Eyelid lesion or rash'}

import os
# dict_count_files_train = {}
ls_temp1 = ['1','108','11','159','177','181','205','22','251','31','37','39','4','5','50','51','74']
# folder path
for i in ls_temp1:
  dir_path = r'/content/drive/MyDrive/final_ee_data_final/train/S'+str(i)
  count = 0
    # Iterate directory
  for path in os.listdir(dir_path):
        # check if current path is a file
        if os.path.isfile(os.path.join(dir_path, path)):
          count += 1
  dict_count_files_train[d2['S'+str(i)]] = count
  print('S'+str(i)+' '+'File count:', count)

dict_count_files_train['Swollen eye']

"""# Data

# Train Data
"""

import pickle

with open('/content/drive/MyDrive/goal_set.p','rb') as f:
    train_data = pickle.load(f)

s_list = ['Swollen eye',  'Mouth ulcer',  'Skin dryness, peeling, scaliness, or roughness',  'Neck swelling',  'Skin irritation', 'Swollen or red tonsils', 'Knee swelling', 'Redness in ear', 'Dry or flaky scalp', 'Skin rash', 'Itchy eyelid', 'Hand or finger lump or mass', 'Skin growth', 'Eye redness', 'Foot or toe swelling', 'Lip swelling',  'Eyelid lesion or rash']
# print(s_list.index('Neck swelling'))
l = []
temp = []
sno = []
sno_dict = {}
for symptom in s_list:
  for i in range(24000):
    traind = train_data['train'][i]['goal']
    # print(traind)
    try:
      traind.pop("request_slots")
    except:
      pass
    for key in traind:
      x = traind[key]
      # print(x)
      # Swollen eye
      for key2 in x:
        # print(key2)
        if key2 == symptom:
          sno.append(i)
          break
  if len(sno)>0:
    sno_dict[symptom] = sno[:dict_count_files_train[symptom]]
  sno = []

for i in sno_dict:
  # print(sno_dict[i])
  print(i, len(sno_dict[i]))

fs = {}
cnt = 0
for symptom in sno_dict:
  # print(symptom)
  for i in sno_dict[symptom]:
    td = train_data['train'][i]['goal']
    # print(td)
    for key in traind:
      x = td[key]
      # print(x)
      # Swollen eye
      for key2 in x:
        # print(key2)
        if key2==symptom:
          break
        temp.append(key2)

    l.append([temp, symptom, cnt])
    temp = []
  cnt = cnt + 1
  fs[symptom] = l
  l = []
# print(sno_dict['Swollen eye'])
print(fs['Mouth ulcer'])
print(len(fs['Mouth ulcer']))
# print(len(fs))

print(train_data['train'])

print(fs['Swollen eye'])

for i in fs:
  # print(fs[i])
  for j in fs[i]:
    if j[0] == []:
      fs[i].remove(j)
      print('yes')

for i in fs:
  # print(sno_dict[i])
  print(i, len(fs[i]))

import matplotlib.pyplot as plt
import os
import tensorflow as tf
import matplotlib
matplotlib.style.use('ggplot')

IMAGE_SHAPE = (224, 224)
TRAINING_DATA_DIR = '/content/drive/MyDrive/final_ee_data_final/train'
VALID_DATA_DIR = '/content/drive/MyDrive/final_ee_data_final/test'

import copy 
final_data  = copy.deepcopy(fs)

print(len(final_data))

print(final_data)

import os
import re

def sorted_alphanumeric(data):
    convert = lambda text: int(text) if text.isdigit() else text.lower()
    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] 
    return sorted(data, key=alphanum_key)

bd = {'Swollen eye': 'S1',  'Mouth ulcer': 'S108',  'Skin dryness, peeling, scaliness, or roughness':'S11',  'Neck swelling': 'S159',  'Skin irritation': 'S177', 'Swollen or red tonsils': 'S181', 'Knee swelling' : 'S205', 'Redness in ear' : 'S22', 'Dry or flaky scalp': 'S251', 'Skin rash': 'S31', 'Itchy eyelid' : 'S37', 'Hand or finger lump or mass':'S39', 'Skin growth':'S4', 'Eye redness':'S5', 'Foot or toe swelling':'S50', 'Lip swelling': 'S51',  'Eyelid lesion or rash':'S74'}
for index in final_data:
  path = "/content/drive/MyDrive/final_ee_data_final/train/" + bd[index]
  dirlist = sorted_alphanumeric(os.listdir(path))
  # print(len(dirlist))
  fdirlist = dirlist + dirlist
  # print(len(fdirlist))
  # print(fdirlist)
  symptom = index
  for i in range(len(final_data[symptom])):
    # print(final_data[symptom][i])
    temp_list = final_data[symptom][i]
    # print(temp_list)
    temp_list[1] = bd[index]+'/'+fdirlist[i]
    final_data[symptom][i] = temp_list
    # print(final_data[symptom][i])
    # print('\n\n')

print(len(final_data))

for i in final_data:
  print(final_data[i])
pickle.dump(final_data, open('/content/drive/MyDrive/train_ca_v6_final.p', 'wb'))

"""# Test_data"""

dict_count_files_test = {}

import os
# dict_count_files_train = {}
ls_temp1 = ['1','108','11','159','177','181','205','22','251','31','37','39','4','5','50','51','74']
# folder path
for i in ls_temp1:
  dir_path = r'/content/drive/MyDrive/final_ee_data_final/test/S'+str(i)
  count = 0
    # Iterate directory
  for path in os.listdir(dir_path):
        # check if current path is a file
        if os.path.isfile(os.path.join(dir_path, path)):
          count += 1
  dict_count_files_test[d2['S'+str(i)]] = count
  print('S'+str(i)+' '+'File count:', count)

import pickle

with open('/content/drive/MyDrive/goal_set.p','rb') as f:
    train_data = pickle.load(f)

s_list = ['Swollen eye',  'Mouth ulcer',  'Skin dryness, peeling, scaliness, or roughness',  'Neck swelling',  'Skin irritation', 'Swollen or red tonsils', 'Knee swelling', 'Redness in ear', 'Dry or flaky scalp', 'Skin rash', 'Itchy eyelid', 'Hand or finger lump or mass', 'Skin growth', 'Eye redness', 'Foot or toe swelling', 'Lip swelling',  'Eyelid lesion or rash']
# print(s_list.index('Neck swelling'))
l = []
temp = []
sno = []
sno_dict = {}
for symptom in s_list:
  for i in range(6000):
    traind = train_data['test'][i]['goal']
    # print(traind)
    try:
      traind.pop("request_slots")
    except:
      pass
    for key in traind:
      x = traind[key]
      # print(x)
      # Swollen eye
      for key2 in x:
        # print(key2)
        if key2 == symptom:
          sno.append(i)
          break
  if len(sno)>0:
    sno_dict[symptom] = sno[:dict_count_files_test[symptom]]
  sno = []

for i in sno_dict:
  # print(sno_dict[i])
  print(i, len(sno_dict[i]))

fs = {}
cnt = 0
for symptom in sno_dict:
  # print(symptom)
  for i in sno_dict[symptom]:
    td = train_data['test'][i]['goal']
    # print(td)
    for key in traind:
      x = td[key]
      # print(x)
      # Swollen eye
      for key2 in x:
        # print(key2)
        if key2==symptom:
          break
        temp.append(key2)

    l.append([temp, symptom, cnt])
    temp = []
  cnt = cnt + 1
  fs[symptom] = l
  l = []
# print(sno_dict['Swollen eye'])
print(fs['Mouth ulcer'])
print(len(fs['Mouth ulcer']))
# print(len(fs))

print(fs['Swollen eye'])

for i in fs:
  # print(fs[i])
  for j in fs[i]:
    if j[0] == []:
      fs[i].remove(j)
      print('yes')
big_list = []
fdict = {}
for i in fs:
  # print(sno_dict[i])
  fdict[i] = len(fs[i])
  print(i, len(fs[i]))
  big_list.append(i)
print(fdict)
pickle.dump(fdict, open('SD_test_distribution.p', 'wb'))

import matplotlib.pyplot as plt
import os
import tensorflow as tf
import matplotlib
matplotlib.style.use('ggplot')

import copy 
final_data  = copy.deepcopy(fs)

import os
import re

def sorted_alphanumeric(data):
    convert = lambda text: int(text) if text.isdigit() else text.lower()
    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] 
    return sorted(data, key=alphanum_key)

bd = {'Swollen eye': 'S1',  'Mouth ulcer': 'S108',  'Skin dryness, peeling, scaliness, or roughness':'S11',  'Neck swelling': 'S159',  'Skin irritation': 'S177', 'Swollen or red tonsils': 'S181', 'Knee swelling' : 'S205', 'Redness in ear' : 'S22', 'Dry or flaky scalp': 'S251', 'Skin rash': 'S31', 'Itchy eyelid' : 'S37', 'Hand or finger lump or mass':'S39', 'Skin growth':'S4', 'Eye redness':'S5', 'Foot or toe swelling':'S50', 'Lip swelling': 'S51',  'Eyelid lesion or rash':'S74'}
for index in final_data:
  path = "/content/drive/MyDrive/final_ee_data_final/test/" + bd[index]
  dirlist = sorted_alphanumeric(os.listdir(path))
  print(len(dirlist))
  fdirlist = dirlist + dirlist
  print(len(fdirlist))
  print(fdirlist)
  symptom = index
  for i in range(len(final_data[symptom])):
    print(final_data[symptom][i])
    temp_list = final_data[symptom][i]
    temp_list[1] = bd[index]+'/'+fdirlist[i]
    final_data[symptom][i] = temp_list
    print(final_data[symptom][i])
    print('\n\n')

for i in final_data:
  print(final_data[i])
pickle.dump(final_data, open('/content/drive/MyDrive/test_ca_v6.p', 'wb'))



"""# Model"""

# !pip install transformers

import io
import re
from transformers import AutoTokenizer
import tensorflow as tf
import tensorflow_hub as hub
import numpy as np
import pandas as pd
from tensorflow import keras
import matplotlib.pyplot as plt
from transformers import pipeline

#Bert
model_name = "emilyalsentzer/Bio_ClinicalBERT"
tokenizer = AutoTokenizer.from_pretrained(model_name)
classifier = pipeline('feature-extraction',model=model_name, tokenizer=tokenizer)

def lambda_func(row):
    tokens = tokenizer(row[0],padding=True,truncation=True,return_tensors="pt")
    if len(tokens['input_ids'])>512:
        tokens = re.split(r'\b', row[0])
        tokens= [t for t in tokens if len(t) > 0 ]
        row[0] = ''.join(tokens[:512])
    row['vectors'] = classifier(row[0])[0][0]        
    return row

def process(progress_notes):     
    progress_notes = progress_notes.apply(lambda_func, axis=1)
    return progress_notes

def embedding(emb): 
  pn=pd.DataFrame([emb])
  pn = process(pn)
  pn = pn['vectors'][0]
  pn = np.array(pn)
  return pn

array = embedding('cold')
print(len(array))

l = ['1','2']
print(l[-6:])
print('[sep]'.join(l))
dic = {'3':'h'}
for i in l:
  if i in dic:
    print('yes')

import torch
import torch.nn as nn  # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions
import torch.optim as optim  # For all Optimization algorithms, SGD, Adam, etc.
import torchvision.transforms as transforms  # Transformations we can perform on our dataset
import torchvision
import os
import pandas as pd
from skimage import io
from torch.utils.data import (
    Dataset,
    DataLoader,
)  # Gives easier dataset managment and creates mini batches
import albumentations as A
from PIL import Image

def preprocesser_data(train_data, test_data):
  
  d2 = {'S1':'Swollen eye', 'S108':'Mouth ulcer', 'S11':'Skin dryness, peeling, scaliness, or roughness', 
          'S159':'Neck swelling', 'S177': 'Skin irritation','S181':'Swollen or red tonsils', 
          'S205':'Knee swelling', 'S22': 'Redness in ear' , 'S251': 'Dry or flaky scalp','S31': 'Skin rash', 
          'S37':'Itchy eyelid','S39': 'Hand or finger lump or mass','S4': 'Skin growth','S5' : 'Eye redness', 
          'S50':'Foot or toe swelling', 'S51':'Lip swelling', 'S74':  'Eyelid lesion or rash'}
  symptoms_lst = ['S1','S108','S11','S159','S177','S181','S205','S22','S251','S31','S37','S39','S4','S5','S50','S51','S74']
  dict_temp = {'S1':6,'S4':6,'S5':6,'S11':8,'S22':8,'S31':8,'S37':8,'S39':8,'S50':8,'S51':8,'S74':8,'S108':10,'S159':10,'S177':10,'S181':10,'S205':10,'S251':10}
  # symptom = d2[_symptom_]
  # temp_class = train_data[symptom][0][2]
  for i in symptoms_lst:
    symptom = d2[i]
    # temp_class = train_data[symptom][0][2]
    for j in train_data[symptom]:
    # print(final_data_s1[i][1])
      # print(dict_temp[i])
      # print(j[1][dict_temp[i]])
      j[2] = (int(j[1][dict_temp[i]])-1)
    # print(i[2])
  ls1_train = []
  ls2_train = []
  ls3_train = []
  temp_dic_train = {}
  for i in symptoms_lst:
    symptom = d2[i]

    for i in train_data[symptom]:
      if i[2]==0:
        ls1_train.append(i)
      elif i[2] == 1:
        ls2_train.append(i)
      elif i[2] == 2:
        ls3_train.append(i)
    
  temp_dic_train['A'+str(1)] = ls1_train
  temp_dic_train['A'+str(2)] = ls2_train
  temp_dic_train['A'+str(3)] = ls3_train
  for i in symptoms_lst:
    symptom = d2[i]
    for j in test_data[symptom]:
      j[2] = (int(j[1][dict_temp[i]])-1)
  ls1_test = []
  ls2_test = []
  ls3_test = []
  temp_dic_test = {}
  for i in symptoms_lst:
    symptom = d2[i]
    for i in test_data[symptom]:
      if i[2]==0:
        ls1_test.append(i)
      elif i[2] == 1:
        ls2_test.append(i)
      elif i[2] == 2:
        ls3_test.append(i)
      
  temp_dic_test['A'+str(1)] = ls1_test
  temp_dic_test['A'+str(2)] = ls2_test
  temp_dic_test['A'+str(3)] = ls3_test
  return temp_dic_train, temp_dic_test

import pickle
with open('/content/drive/MyDrive/train_ca_v5_final.p','rb') as f:
    train_data_temp = pickle.load(f)
with open('/content/drive/MyDrive/test_ca_v5.p','rb') as f:
    test_data_temp = pickle.load(f)
train_data, test_data = preprocesser_data(train_data_temp, test_data_temp)
print(len(train_data))
# print(train_data)
print(len(test_data))
train_list = []
test_list = []
for i in train_data:
  for j in train_data[i]:
    train_list.append(j)
for i in test_data:
  for j in test_data[i]:
    test_list.append(j)
# print(train_list[23])
# print(test_list[12])
# for i in range(len(test_list)):
#   print(test_list[i])
# s_list = ['Swollen eye',  'Mouth ulcer',  'Skin dryness',  'Neck swelling',  'Skin irritation', 'Swollen tonsils', 'Knee swelling', 'Redness in ear', 'Dry scalp', 'Skin rash', 'Itchy eyelid', 'Hand lump', 'Skin growth', 'Eye redness', 'Foot swelling', 'Lip swelling',  'Eyelid rash']
print(train_list)

print(train_data_temp)

print(len(train_data))

print(len(test_data))

print(train_data)

class Symptoms(Dataset):
    def __init__(self, data_list, root_dir,context_no, transform=None):
        self.annotations = data_list
        self.context_no = context_no
        # self.annotations = pd.read_csv(csv_file)
        self.root_dir = root_dir
        self.transform = transform

    def __len__(self):
        return len(self.annotations)

    def embedding(emb): 
        pn=pd.DataFrame([emb])
        pn = process(pn)
        pn = pn['vectors'][0]
        pn = np.array(pn)
        return pn

    def __getitem__(self, index):
        img_path = os.path.join(self.root_dir, self.annotations[index][1])
        
        image = np.array(Image.open(img_path).convert('RGB'))
        y_label = torch.tensor(int(self.annotations[index][2]))
        emb_list = self.annotations[index][0]
        x = self.context_no
        temp_list = emb_list[-x:]
        change_dic = {'Skin dryness, peeling, scaliness, or roughness': 'Skin dryness',  
                        'Swollen or red tonsils':'Swollen tonsils',  'Dry or flaky scalp': 'Dry scalp', 
                        'Hand or finger lump or mass': 'Hand lump', 
                       'Foot or toe swelling': 'Foot swelling',  'Eyelid lesion or rash': 'Eyelid rash'}
        for i in range(len(temp_list)):
          if temp_list[i] in change_dic:
            temp_list[i] = change_dic[temp_list[i]]
        seperator = '$'
        final_embedding_string = seperator.join(temp_list)
        # print(final_embedding_string)
        emb_array = embedding(final_embedding_string)
        emb_array = torch.tensor(emb_array)
        ls1 = []
        ls1.append(self.annotations[index][1])
        transform = A.Compose(
        [A.Resize(width=224, height=224)],
        )
        augmentations = transform(image=image)
        image = augmentations["image"]
        

        if self.transform:
            image = self.transform(image)
        
            
        # return (image, y_label, emb_array)
        return (image, y_label, emb_array)

print(len(train_list))

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Hyperparameters
in_channel = 3
num_classes = 3

batch_size = 32


# Load Data
train_dataset = Symptoms(
    data_list = train_list,
    root_dir='/content/drive/MyDrive/final_ee_data_final/train',
    context_no = 3,
    transform=transforms.ToTensor(),
)
test_dataset = Symptoms(
    data_list = test_list,
    root_dir='/content/drive/MyDrive/final_ee_data_final/test',
    context_no = 3,
    transform=transforms.ToTensor(),
)

train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)

print(len(test_dataset))

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

import sys
from tqdm import tqdm
import time
import copy

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device

import torchvision.models as models
model = torch.hub.load('pytorch/vision:v0.10.0','vgg19',pretrained = True)
for param in model.parameters():
    param.requires_grad = False
model = model.to(device)

import torch.nn.functional as F
class Model(nn.Module):
    def __init__(self, original_model):
        super(Model, self).__init__()
        self.features = nn.Sequential(*list(original_model.children())[:-1])
        
        # n_inputs = model.classifier.in_features
        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, 
        #padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
        # self.layer =  nn.Linear(25088, 100)
        # self.linear = nn.Linear(20368,1000)
        # self.linear1 = nn.Linear(1000,50)
        # self.linear2 = nn.Linear(200,50)
        # self.linear3 = nn.Linear(250,100)
        
        
    
        # self.output = nn.Linear(50, 17)
        
        # self.dropout = nn.Dropout(0.3)
        # self.dropout1 = nn.Dropout(0.2)
        # self.dropout2 = nn.Dropout(0.1)
        
        # self.layer = nn.Linear(768, 32)
        self.conv1 = nn.Conv2d(512, 1024,  3, 1,  bias=False, padding="valid")
        self.conv2 = nn.Conv2d(1024, 512,  3, 1, bias=False, padding="valid")
        self.conv3 = nn.Conv2d(512, 512,  3, 2, bias=False, padding="valid")
        # self.pool = nn.MaxPool2d(4, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)
        self.linear = nn.Linear(512, 512)
        #1280

        # self.linear2 = nn.Linear(512,256)
        # self.linear3 = nn.Linear(256,128)
        
        self.output = nn.Linear(512,3)

        
    def forward(self, x, e):
        # print(x.shape)
        x = self.features(x)
        # print(x.shape)
        # print(x.shape)
        x = F.relu(self.conv1(x))
        # print(x.shape)
        x = F.relu(self.conv2(x))
        # print(x.shape)
        x = F.relu(self.conv3(x))
        # print(x.shape)
        x = F.max_pool2d(x, kernel_size=x.size()[2:])
        # print(x.shape)

        a,b,c,d = x.shape
        x = torch.reshape(x, (a, -1))
        

        x = F.relu(self.linear(x))

        
        # x = torch.cat([x, e], dim=1)

        # x = F.relu(self.linear1(x))

        
        
        x = self.output(x)

        return x

model_final = Model(model).to(device)

print(model_final)

print(model)

from tqdm import tqdm
import numpy as np
from PIL import Image
from torchvision import transforms
# Loss and optimizer
learning_rate = 1e-3
criterion = nn.CrossEntropyLoss()

optimizer = optim.Adam(model_final.parameters(), lr=learning_rate, weight_decay = 1e-5)

# decayRate = 0.96
# my_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)

num_epochs = 30

def check_accuracy(loader, model):
    num_correct = 0
    num_samples = 0
    model.eval()

    with torch.no_grad():
        # for x, y, e in loader:
        ls_1 = []
        for x, y, e in loader:
            # x = x.to(device=device)

            
            x = x.to(device=device)
                # model.to('cuda')
            e = e.to(device=device)
            y = y.to(device=device)

            scores = model(x, e.float())
            
            
            #scores = model(x)
            _, predictions = scores.max(1)
            num_correct += (predictions == y).sum()
            num_samples += predictions.size(0)


    model.train()
    return num_correct/num_samples

def save_checkpoint(model, optimizer, filename):
    print("=> Saving checkpoint")
    checkpoint = {
        "state_dict": model.state_dict(),
        "optimizer": optimizer.state_dict(),
    }
    torch.save(checkpoint, filename)


# Train Network
max_accuracy = 0
dict_train_test = {}
for epoch in range(num_epochs):
    print("\n\nCurrent EPOCH:", epoch)
    # for batch_idx, (data, targets, embeddings) in enumerate(tqdm(train_loader)):
    train_ls = []
    for batch_idx, (data, targets, embeddings) in enumerate(tqdm(train_loader)):
        # Get data to cuda if possible
        
        data = data.to(device=device)
        embeddings = embeddings.to(device=device)
        targets = targets.to(device=device)
        
        # print(data)
        # print(targets)
        # print(embeddings)
        # forward
        scores= model_final(data, embeddings.float())
        #print(scores)
        # train_ls.append(ls_1)
        loss = criterion(scores, targets)
        if batch_idx==41:
          print(f'Loss at step {batch_idx} = ',loss)
        if batch_idx%5 ==0:
          print(f'Loss at step {batch_idx} = ',loss)

        # backward
        optimizer.zero_grad()
        loss.backward()

        # gradient descent or adam step
        optimizer.step()
    
    curr_accuracy = check_accuracy(test_loader, model_final)
    # if curr_accuracy > max_accuracy:
    #   fname  = '/content/drive/MyDrive/multimodal_aaai_context/context 4/'+'c4_convnext_severity_checkpoint'+'.pth.tar'
    #   save_checkpoint(model_final, optimizer, fname)
    #   max_accuracy = curr_accuracy


    print(f"Accuracy at EPOCH {epoch} on test set: {curr_accuracy*100:.2f}")

learning_rate = 1e-3
criterion = nn.CrossEntropyLoss()

optimizer = optim.Adam(model_final.parameters(), lr=learning_rate, weight_decay = 1e-5)

def load_checkpoint(checkpoint_file, model, optimizer, lr):
    print("=> Loading checkpoint")
    checkpoint = torch.load(checkpoint_file, map_location=device)
    model.load_state_dict(checkpoint["state_dict"])
    optimizer.load_state_dict(checkpoint["optimizer"])

    # If we don't do this then it will just have learning rate of old checkpoint
    # and it will lead to many hours of debugging \:
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr
path = '/content/drive/MyDrive/multimodal_AAAI_context_severity/context 3/c3_severity_checkpoint.pth.tar'
load_checkpoint(path, model_final,optimizer, learning_rate)

train_ls = []

print(len(train_ls))

test_ls = []

import numpy
y_pred = []
y_true = []
dict_final_pred = {}
def check_accuracy(loader, model,y_true,y_pred):
    num_correct = 0
    num_samples = 0
    model.eval()

    with torch.no_grad():
        # for x, y, e in loader:
        for x, y, e in loader:
            # input_image = x
            # preprocess = transforms.Compose([
            #     transforms.Resize(256),
            #     transforms.CenterCrop(224),
            #     # transforms.ToTensor(),
            #     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            # ])
            # x = preprocess(input_image)
            x = x.to(device=device)
            e = e.to(device=device)
            y = y.to(device=device)
            # for i in p:
            #   for j in i:
            #     str1 = ''
            #     str1 = str1 + j
            
            # print(y.shape)
            lis1 = numpy.array(y.cpu())
            lis1 = lis1.tolist()
            # print(lis1)
            y_true = y_true + lis1
            # print(y_true)
            # scores = model(x, e.float())
            scores = model(x,e.float())
            m = nn.Softmax(dim=1)
            output = m(scores)
            # print(output)
            # print(str1)
            
            # print(scores)
            _, predictions = output.max(1)
            d1 = {0: 'S1', 1 :'S108', 2:'S11', 3:'S159', 4:'S177', 5:'S181', 6:'S205', 7:'S22', 8:'S251', 9:'S31', 10:'S37'
                  , 11:'S39', 12:'S4', 13: 'S5', 14:'S50', 15:'S51', 16:'S74'}
            d2 = {'S1':'Swollen eye', 'S108':'Mouth ulcer', 'S11':'Skin dryness, peeling, scaliness, or roughness', 
                  'S159':'Neck swelling', 'S177': 'Skin irritation','S181':'Swollen or red tonsils', 
                  'S205':'Knee swelling', 'S22': 'Redness in ear' , 'S251': 'Dry or flaky scalp','S31': 'Skin rash', 
                  'S37':'Itchy eyelid','S39': 'Hand or finger lump or mass','S4': 'Skin growth','S5' : 'Eye redness', 
                  'S50':'Foot or toe swelling', 'S51':'Lip swelling', 'S74':  'Eyelid lesion or rash'}
            # print(d2[d1[int(predictions[0])]])
            # test_ls.append((str1,d2[d1[int(predictions[0])]],output))
            # print(predictions)
            lis2 = numpy.array(predictions.cpu())
            lis2 = lis2.tolist()
            y_pred = y_pred + lis2
    
              
            num_correct += (predictions == y).sum()
            num_samples += predictions.size(0)


    model.train()
    return num_correct/num_samples, y_true, y_pred

a, y_true, y_pred =check_accuracy(test_loader, model_final,y_true, y_pred)
print(f"Accuracy on test set: {a*100:.2f}")
print(y_true)
print(len(y_true))
print(y_pred)
print(len(y_pred))
for i in range(len(y_true)):
  print(y_true[i],'  ', y_pred[i],' ', i)

# k = ['S159-A1','S159-A2','S159-A3']
k = ['Severe','Moderate','Mild']

from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
import numpy as np
import sklearn
from sklearn.metrics import confusion_matrix
import seaborn as sns

print('true classes:',y_true)

score = accuracy_score(y_true, y_pred)
print('Classification Report: ',classification_report(y_true,y_pred))
cm = confusion_matrix(y_true, y_pred)
f,ax= plt.subplots(figsize=(15, 15))
sns.heatmap(cm, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation

# labels, title and ticks
ax.set_xlabel('Predicted labels',fontsize = 28);ax.set_ylabel('True labels',fontsize = 28);
ax.set_title('Confusion Matrix');
classes_rev = []

classes = k
l = len(k)
for i in range(1,l+1):
    classes_rev = classes_rev + [classes[l-i]]
ax.xaxis.set_ticklabels(classes,fontsize = 5, );ax.yaxis.set_ticklabels(classes, fontsize = 5);
ax.figure.savefig("hybrid_cnn.png")
print('Accuracy: ', score)
print('F1',sklearn.metrics.f1_score(y_true, y_pred, average='weighted'))